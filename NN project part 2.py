# -*- coding: utf-8 -*-
"""NN project part 2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1WfRoTSdkfhsIOdy0MVML29MHWZXutH9z
"""

pip install torch torchvision fiftyone albumentations matplotlib

import fiftyone as fo
import fiftyone.zoo as foz
import torch
from torchvision.models.detection import FasterRCNN
from torchvision.models.detection.backbone_utils import mobilenet_backbone
from torchvision.models.detection.backbone_utils import BackboneWithFPN, LastLevelMaxPool
from torchvision.models.detection.faster_rcnn import FastRCNNPredictor
from torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights
from torchvision.transforms import functional as F
from torchvision.ops import misc as misc_nn_ops
from PIL import Image
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, Dataset
import torchvision
import torch
import torch.optim as optim
from torch.utils.data import DataLoader



train_dataset = foz.load_zoo_dataset(
    "coco-2017",
    split="train",
    label_types=["detections"],
    classes=["person", "car"],
    max_samples=100,
)

val_dataset = foz.load_zoo_dataset(
    "coco-2017",
    split="validation",
    label_types=["detections"],
    classes=["person", "car"],
    max_samples=25,
)

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, fiftyone_dataset):
        self.samples = []
        # Iterate through the FiftyOne dataset and store filepaths and annotations
        for sample in fiftyone_dataset:
            # Convert FiftyOne detections to the format expected by the model
            boxes = []
            labels = []
            for detection in sample.ground_truth.detections:
                bbox = detection.bounding_box
                # Convert from normalized [x_min, y_min, width, height] to [x_min, y_min, x_max, y_max]
                x_min = bbox[0] * sample.metadata.width
                y_min = bbox[1] * sample.metadata.height
                x_max = x_min + (bbox[2] * sample.metadata.width)
                y_max = y_min + (bbox[3] * sample.metadata.height)
                boxes.append([x_min, y_min, x_max, y_max])
                labels.append(1 if detection.label == 'person' else 2)  # Update according to your class labels

            self.samples.append((sample.filepath, boxes, labels))

    def __getitem__(self, idx):
        filepath, boxes, labels = self.samples[idx]
        image = Image.open(filepath).convert("RGB")
        image = F.to_tensor(image)
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels

        return image, target

    def __len__(self):
        return len(self.samples)



def get_model(num_classes):
    # load an instance segmentation model pre-trained on COCO
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) #Change from ResNet to MobileNet

    # get number of input features for the classifier
    in_features = model.roi_heads.box_predictor.cls_score.in_features

    # replace the pre-trained head with a new one
    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)

    return model

# Adjust num_classes to include the background
num_classes = 3  # 2 classes (person, cars) + background
model = get_model(num_classes)

train_ds = CustomDataset(train_dataset)
val_ds = CustomDataset(val_dataset)

# DataLoader
train_loader = DataLoader(train_ds, batch_size=3, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))
val_loader = DataLoader(val_ds, batch_size=3, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))

# Assume model, train_loader are defined
# Move model to the appropriate device
device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')
model.to(device)

# Parameters
num_epochs = 10
lr = 0.005
momentum = 0.9
weight_decay = 0.0005

# Define the optimizer
optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)

# Learning rate scheduler - not a must
lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

# Training loop
for epoch in range(num_epochs):
    model.train()  # Set model to training mode

    running_loss = 0.0
    for images, targets in train_loader:
        # Move images and targets to the right device
        images = list(image.to(device) for image in images)
        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

        # Zero the parameter gradients
        optimizer.zero_grad()

        # Forward pass
        loss_dict = model(images, targets)

        # Total loss
        losses = sum(loss for loss in loss_dict.values())
        print(losses)

        # Backward pass + optimize
        losses.backward()
        optimizer.step()

        # Print statistics
        running_loss += losses.item()

    # Step the learning rate scheduler
    lr_scheduler.step()

    # Print loss for the epoch
    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader)}')

print('Finished Training')

import cv2
import torch
from torchvision.transforms import functional as F
from PIL import Image

def detect_objects(frame, model, device):
    # Convert the frame from OpenCV format (BGR) to PIL format (RGB)
    pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))

    # Transform the image for the model
    image = F.to_tensor(pil_image).unsqueeze(0).to(device)

    # Perform inference
    model.eval()
    with torch.no_grad():
        prediction = model(image)

    # Create a mapping for your labels
    label_map = {1: 'person', 2: 'car'}

    # Draw bounding boxes on the frame
    for element in range(len(prediction[0]['boxes'])):
        boxes = prediction[0]['boxes'][element].cpu().numpy().astype(int)
        score = prediction[0]['scores'][element].cpu().numpy()
        label = prediction[0]['labels'][element].cpu().numpy()

        label = int(label)

        # Filter out lower scores
        if score > 0.95:  # threshold
            start_point = (boxes[0], boxes[1])
            end_point = (boxes[2], boxes[3])
            color = (0, 255, 0) if label == 1 else (0, 0, 255)  # Green for person, red for car
            thickness = 2  # Box thickness

            # Map the label number to its name
            label_name = label_map.get(label)
            # Add the box to the frame
            frame = cv2.rectangle(frame, start_point, end_point, color, thickness)

            # Optionally, add label and score text above the box
            cv2.putText(frame, f'{label_name}: {score:.2f}', (boxes[0], boxes[1]-10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, thickness)

    return frame

from google.colab import drive
drive.mount('/content/drive')

input_video_path = '/content/drive/My Drive/Colab/NYC_street.mp4'
print ("hey")
output_video_path = '/content/drive/My Drive/Colab/output_NYC_street.mp4'

def process_video(input_video_path, output_video_path, model, device):
    # Open the input video
    cap = cv2.VideoCapture(input_video_path)

    # Get video properties
    fps = int(cap.get(cv2.CAP_PROP_FPS))
    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))

    # Define the codec and create VideoWriter object
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')  # or 'XVID'
    out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))

    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break

        # Detect objects in the frame
        output_frame = detect_objects(frame, model, device)

        # Write the frame into the output video
        out.write(output_frame)

    # Release everything when done
    cap.release()
    out.release()
    cv2.destroyAllWindows()

# Assuming the model and device are already set
process_video(input_video_path, output_video_path, model, device)